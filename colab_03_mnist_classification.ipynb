{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNIST + Convolutional Neural Network\n",
    "In this notebook we will train a simple convolutional neural networks (CNNs) for classifying hand-written digit images from MNIST data set.  We assume you already know how to access MNIST data set using tensorflow API.\n",
    "\n",
    "**<font color=\"red\">Warning</font>**\n",
    "If you already know higher level APIs like [keras](https://keras.io) and [slim](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/slim) (if you don't, ignore this warning), you see we avoid using them in this notebook purely for educational purpose. You can see the [next tutorial](http://deeplearnphysics.org/Blog/colab-tutorials/2018-03-02-Colab-04-MNIST-Classification-Slim.html) instead.\n",
    "\n",
    "### Preparation?\n",
    "You can run this notebook on [google colaboratory](https://colab.research.google.com) with a free K80 GPU. If you are not familiar with MNIST, you can look at [this example](http://deeplearnphysics.org/Blog/colab-tutorials/2018-03-02-Colab-02-MNIST.html). If usefl, also [here](http://deeplearnphysics.org/Blog/colab-tutorials/2018-03-02-Colab-01-TFIntro.html) is tensorflow introduction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "# Import tensorflow\n",
    "import tensorflow as tf\n",
    "# Import tensorflow's MNIST data handle\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets('MNIST_data', one_hot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use a pedantic way to construct a computational graph in this notebook because it is more educational. Just remember that you can use a higher level APIs (tf.layers, tf.contrib.slim, keras, etc.) to make an equivalent or more complicated computation graph with much less number of steps.\n",
    "\n",
    "## Define layers: convolution, pooling, and fully-connected \n",
    "We will use 3 type of layers (convolution, pooling, and fully-connected layers), and there will be multiple instance of each type. To avoid typing too many texts, it is a common practice to define a function that can create a layer with a few parameters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import xavier_initializer, a type of initializer we will use for weights\n",
    "from tensorflow.contrib.layers import xavier_initializer\n",
    "\n",
    "# Convolution layer\n",
    "def conv2d(input_tensor, name, num_filter, kernel, stride):\n",
    "    # From the input tensor, get the number of feature maps\n",
    "    # The shape of an input tensor is [N,H,W,C] where...\n",
    "    #    N = number of images in 1 batch\n",
    "    #    H = height of input tensor\n",
    "    #    W = width of input tensor\n",
    "    #    C = channel count of input tensor\n",
    "    # The number of feture maps = C, so we access the last element\n",
    "    num_input = input_tensor.get_shape()[-1].value\n",
    "\n",
    "    # Create graph nodes within this layer's scope\n",
    "    with tf.variable_scope(name):\n",
    "        # Create the weights for all neurons (filters)\n",
    "        weights = tf.get_variable(name='%s_weights' % name,\n",
    "                                  shape=[kernel[0],kernel[1],num_input,num_filter],\n",
    "                                  dtype=tf.float32,\n",
    "                                  initializer=xavier_initializer())\n",
    "        # Create the biases for all neurons (filters)\n",
    "        biases  = tf.get_variable(name='%s_biases' % name,\n",
    "                                  shape=[num_filter],\n",
    "                                  dtype=tf.float32,\n",
    "                                  initializer=tf.constant_initializer(0.1))\n",
    "        # Define convolution operation using weights\n",
    "        conv = tf.nn.conv2d(input=input_tensor,\n",
    "                            filter=weights,\n",
    "                            strides=(1, stride[0], stride[1], 1),\n",
    "                            padding='SAME')\n",
    "        \n",
    "        # Define activation operation with an argument = neuron sum\n",
    "        activation = tf.nn.relu(tf.nn.bias_add(conv, biases))\n",
    "\n",
    "        # Register variables to be monitored in tensorboard\n",
    "        tf.summary.histogram('%s_weights' % name, weights)\n",
    "        tf.summary.histogram('%s_biases' % name, biases)\n",
    "        tf.summary.histogram('%s_activation' % name, activation)\n",
    "\n",
    "    return activation\n",
    "    \n",
    "# Pooling layer\n",
    "def max_pool(input_tensor,name,kernel,stride):\n",
    "    # Create graph nodes within this layer's scope\n",
    "    with tf.variable_scope(name):\n",
    "\n",
    "        pool = tf.nn.max_pool(value=input_tensor,\n",
    "                              ksize=[1, kernel[0], kernel[1], 1],\n",
    "                              strides=[1, stride[0], stride[1], 1],\n",
    "                              padding='SAME')\n",
    "    return pool\n",
    "\n",
    "# Fully connected layer\n",
    "def fully_connected(input_tensor, name, num_output, activation=None):\n",
    "    # Flatten the input tensor to 1D array \n",
    "    shape=input_tensor.get_shape()\n",
    "    flat_size = 1\n",
    "    for index in range(len(shape)-1):\n",
    "        flat_size *= shape[1-len(shape)+index].value\n",
    "    input_tensor = tf.reshape(input_tensor, [-1,flat_size])\n",
    "\n",
    "    # Create graph nodes within this layer's scope\n",
    "    with tf.variable_scope(name):\n",
    "        # Weights for neurons\n",
    "        weights = tf.get_variable(name='%s_weights' % name,\n",
    "                                  shape=[input_tensor.get_shape()[-1].value, num_output],\n",
    "                                  dtype=tf.float32,\n",
    "                                  initializer=xavier_initializer())\n",
    "        # Biases for neurons\n",
    "        biases  = tf.get_variable(name='%s_biases' % name,\n",
    "                                  shape=[num_output],\n",
    "                                  dtype=tf.float32,\n",
    "                                  initializer=tf.constant_initializer(0.1))\n",
    "        \n",
    "        # Compute neuron sums\n",
    "        result = tf.matmul(input_tensor,weights) + biases\n",
    "        \n",
    "        # Activation\n",
    "        if activation:\n",
    "            result = activation(result)\n",
    "        \n",
    "        # Register variables to be monitored in tensorboard\n",
    "        tf.summary.histogram('%s_weights' % name, weights)\n",
    "        tf.summary.histogram('%s_biases' % name, biases)\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Design the network\n",
    "By having those functions defining the details, now we can have a simple description of the overall network design. Let's first define our input place holders. They serve to feed input data image, correct prediction labels, and also a dropout fraction for a fully connected layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Network inputs\n",
    "with tf.variable_scope('input'):\n",
    "    images    = tf.placeholder(tf.float32, shape=[None, 784])\n",
    "    labels    = tf.placeholder(tf.float32, shape=[None, 10])\n",
    "    keep_prob = tf.placeholder(tf.float32)\n",
    "    \n",
    "    image2d   = tf.reshape(images, [-1, 28, 28, 1])\n",
    "    # Record a random 10 samples of images in monitoring\n",
    "    tf.summary.image('images',image2d,10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's the full design of the network. It is important to try keeping this trivial so that your friends can have an abstract understanding quickly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Design a network\n",
    "def build(input_tensor, debug=False):\n",
    "    # Design a network\n",
    "    \n",
    "    # Convolution 1\n",
    "    conv1 = conv2d   (input_tensor, 'conv1', 32, [5,5], [1,1])\n",
    "    # Pooling 1\n",
    "    pool1 = max_pool (conv1,        'pool1',     [2,2], [2,2])\n",
    "    # Convolution 2\n",
    "    conv2 = conv2d   (pool1,        'conv2', 64, [5,5], [1,1])\n",
    "    # Pooling 2\n",
    "    pool2 = max_pool (conv2,        'pool2',     [2,2], [2,2])\n",
    "\n",
    "    # FC1\n",
    "    fc1 = fully_connected(pool2, 'fc1', 1024, tf.nn.relu)\n",
    "    with tf.variable_scope('drop_out'):\n",
    "        drop = tf.nn.dropout(fc1, keep_prob)\n",
    "    # FC2\n",
    "    fc2 = fully_connected(drop, 'fc2', 10)\n",
    "    \n",
    "    # Dimension check if debug is True\n",
    "    if debug:\n",
    "        print('Input shape:', input_tensor.shape )\n",
    "        print('After conv1:', conv1.shape        )\n",
    "        print('After pool1:', pool1.shape        )\n",
    "        print('After conv2:', conv2.shape        )\n",
    "        print('After pool2:', pool2.shape        )\n",
    "        print('After fc1  :', fc1.shape          )\n",
    "        print('After fc2  :', fc2.shape          )\n",
    "        \n",
    "    return fc2    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now build it! We enable the debug flag to print out the dimensionality here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Input shape:', TensorShape([Dimension(None), Dimension(28), Dimension(28), Dimension(1)]))\n",
      "('After conv1:', TensorShape([Dimension(None), Dimension(28), Dimension(28), Dimension(32)]))\n",
      "('After pool1:', TensorShape([Dimension(None), Dimension(14), Dimension(14), Dimension(32)]))\n",
      "('After conv2:', TensorShape([Dimension(None), Dimension(14), Dimension(14), Dimension(64)]))\n",
      "('After pool2:', TensorShape([Dimension(None), Dimension(7), Dimension(7), Dimension(64)]))\n",
      "('After fc1  :', TensorShape([Dimension(None), Dimension(1024)]))\n",
      "('After fc2  :', TensorShape([Dimension(None), Dimension(10)]))\n"
     ]
    }
   ],
   "source": [
    "net = build(image2d, debug=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define loss and optimizer\n",
    "The final step before training... is to define a train step operation! Below we define our loss using softmax (multinominal logistic regression) and we use AdamOptimizer. There is no special reason for this choice. Feel free to try out different optimizers!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Train operations\n",
    "with tf.variable_scope(\"train\"):\n",
    "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=labels, logits=net))\n",
    "    train_step = tf.train.AdamOptimizer(1e-4).minimize(loss)\n",
    "    \n",
    "    correct_prediction = tf.equal(tf.argmax(net, 1), tf.argmax(labels, 1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "    \n",
    "    # Monitor these loss and accuracy during training\n",
    "    tf.summary.scalar('loss',loss)\n",
    "    tf.summary.scalar('accuracy',accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a session and tensorboard log\n",
    "Finally, we are almost ready for the training. Let's create a session + log writer that can record everything we added to tf.summary into the training log. We will visualize it later using tensorboard."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Define a session\n",
    "sess = tf.InteractiveSession()\n",
    "\n",
    "# Create log monitor\n",
    "import os\n",
    "if not os.path.isdir('tb_log'): os.makedirs('tb_log')\n",
    "log_writer = tf.summary.FileWriter('tb_log')\n",
    "log_writer.add_graph(sess.graph)\n",
    "summary_op = tf.summary.merge_all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train\n",
    "Ready? Here's the training loop!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0, training accuracy 0\n",
      "step 100, training accuracy 0.72\n",
      "step 200, training accuracy 0.9\n",
      "step 300, training accuracy 0.88\n",
      "step 400, training accuracy 0.98\n",
      "step 500, training accuracy 0.98\n",
      "step 600, training accuracy 0.96\n",
      "step 700, training accuracy 0.94\n",
      "step 800, training accuracy 0.94\n",
      "step 900, training accuracy 0.88\n",
      "step 1000, training accuracy 0.98\n",
      "step 1100, training accuracy 0.96\n",
      "step 1200, training accuracy 1\n",
      "step 1300, training accuracy 0.92\n",
      "step 1400, training accuracy 1\n",
      "step 1500, training accuracy 1\n",
      "step 1600, training accuracy 0.98\n",
      "step 1700, training accuracy 0.96\n",
      "step 1800, training accuracy 1\n",
      "step 1900, training accuracy 1\n",
      "test accuracy 0.9791\n",
      "23.5245790482\n"
     ]
    }
   ],
   "source": [
    "# Let's time this\n",
    "import time\n",
    "start = time.time()\n",
    "\n",
    "# Ready! initialize and train for 5000 steps\n",
    "sess.run(tf.global_variables_initializer())\n",
    "for i in range(2000):\n",
    "\n",
    "    # Fetch data of 50 images\n",
    "    batch = mnist.train.next_batch(50)\n",
    "\n",
    "    # Every 100 steps, compute & print the accuracy of the network's prediction\n",
    "    if i % 100 == 0:\n",
    "        train_accuracy = sess.run(accuracy, feed_dict={images: batch[0], labels: batch[1], keep_prob: 1.0})\n",
    "        print('step %d, training accuracy %g' % (i, train_accuracy))\n",
    "    \n",
    "    # Every 20 steps, compute & record tensorboard log\n",
    "    if i % 20 == 0:\n",
    "        s = sess.run(summary_op, feed_dict={images: batch[0], labels: batch[1], keep_prob: 1.0})\n",
    "        log_writer.add_summary(s,i)\n",
    "\n",
    "    # Every step run training!\n",
    "    sess.run(train_step, feed_dict={images: batch[0], labels: batch[1], keep_prob: 0.5})\n",
    "\n",
    "# Compute & print out the final accuracy\n",
    "batch = mnist.test.next_batch(200)\n",
    "print('test accuracy %g' % sess.run(accuracy, feed_dict={images: batch[0], labels: batch[1], keep_prob: 1.0}))\n",
    "print(time.time() - start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So it took 23 seconds and the accuracy is just below 100%. Not too bad :)\n",
    "\n",
    "## Analysis\n",
    "The results look pretty good: the accuracy is almost 100% correct most of times. But it's _really_ important for you to double-check this by visually checking the results. I cannot stress this point! So let's do analysis. We will dump some MNIST images, correct labels, and prediction by the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction\u001b[91m 7 \u001b[00mwith softmax prob\u001b[94m 0.999986 \u001b[00m\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP4AAAD8CAYAAABXXhlaAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAABfRJREFUeJzt3c+LzXscx/HzvVgoG0QpoaxsyI+FQiElLPEfaDbsxNre\nys4/YGEhG4mFBYsZG1PkV2lWJKXGAiXU1+KuZnHe3zv3+M45M6/HY/vqzHzv4tlHfe53TtO27QDI\n8s+4HwBYesKHQMKHQMKHQMKHQMKHQMKHQKv7/gVN0/gfBWAM2rZthm1OfAgkfAgkfAgkfAgkfAgk\nfAgkfAgkfAgkfAgkfAgkfAgkfAgkfAgkfAgkfAgkfAgkfAgkfAgkfAgkfAgkfAgkfAgkfAgkfAgk\nfAgkfAgkfAgkfAgkfAgkfAgkfAgkfAgkfAgkfAgkfAgkfAgkfAgkfAgkfAgkfAgkfAgkfAgkfAgk\nfAgkfAgkfAgkfAgkfAgkfAgkfAgkfAgkfAgkfAgkfAi0etwPMOnOnTtX7hcuXCj3jx8/lvuPHz/K\n/datW+X+6dOncp+bmyt3MjnxIZDwIZDwIZDwIZDwIZDwIZDwIVDTtm2/v6Bp+v0FPeu6B9+xY8fS\nPMgQX79+LfdXr14t0ZNMnvfv35f79evXy/3Zs2d/83GWXNu2zbDNiQ+BhA+BhA+BhA+BhA+BhA+B\nhA+BvI/fYWpqqtx3795d7m/evCn3Xbt2lfu+ffvK/ejRo+V+8ODBcv/w4UO5b926tdxH8fv373L/\n/PlzuW/ZsqXcu/7bu+75l/s9fsWJD4GED4GED4GED4GED4GED4GED4G8j7/Mbdiwodz37NlT7rOz\ns+V+4MCBRT/Tf9X1nQLv3r0r99evX5f7+vXry/3ixYvlfvPmzXKfdN7HBxYQPgQSPgQSPgQSPgQS\nPgQSPgRyj8/EOnv2bLnfvn273F++fFnux48fL/f5+flyn3Tu8YEFhA+BhA+BhA+BhA+BhA+BhA+B\n3OMzNps3by73Fy9ejPT58+fPl/udO3fKfblzjw8sIHwIJHwIJHwIJHwIJHwIJHwItHrcD0Curr9r\nv2nTpnL/8uVLub99+3bRz5TCiQ+BhA+BhA+BhA+BhA+BhA+BhA+BvI9Pbw4dOlTujx49Kvc1a9aU\n+7Fjx8r9yZMn5b7SeR8fWED4EEj4EEj4EEj4EEj4EEj4EMj7+PTm9OnT5d51T991zz8zM7PoZ+Jf\nTnwIJHwIJHwIJHwIJHwIJHwIJHwI5B6fkaxdu3bodvLkyfKzP3/+LPdr166V+69fv8qd4Zz4EEj4\nEEj4EEj4EEj4EEj4EEj4EMg9PiO5cuXK0G3v3r3lZx88eFDu09PT/+uZ6ObEh0DCh0DCh0DCh0DC\nh0DCh0DCh0BN2/b79fVN0/T7C+jVmTNnyv3u3btDt+/fv5efPXXqVLk/ffq03Km1bdsM25z4EEj4\nEEj4EEj4EEj4EEj4EEj4EMj7+OE2btxY7jdu3Cj3VatWDd3u379fftY9/fg48SGQ8CGQ8CGQ8CGQ\n8CGQ8CGQ8CGQ9/FXuOqefTAYDGZmZsp9//795T43Nzd063rfvvoso/M+PrCA8CGQ8CGQ8CGQ8CGQ\n8CGQ8CGQ9/FXuJ07d5Z71z19l8uXLw/d3NNPLic+BBI+BBI+BBI+BBI+BBI+BBI+BHKPv8xt3769\n3B8+fDjSz7969Wq537t3b6Sfz3g48SGQ8CGQ8CGQ8CGQ8CGQ8CGQ8CGQe/xlbmpqqty3bds20s9/\n/Phxuff9vQz0w4kPgYQPgYQPgYQPgYQPgYQPgYQPgdzjT7jDhw+X+6VLl8q9aYZ+RfpgMOi+h3dP\nvzI58SGQ8CGQ8CGQ8CGQ8CGQ8CGQ8CGQe/wJd+TIkXJft25duXfdw3d9h/23b9/KneXJiQ+BhA+B\nhA+BhA+BhA+BhA+BhA+B3OOvcM+fPy/3EydOlPv8/PzffBwmhBMfAgkfAgkfAgkfAgkfAgkfAgkf\nAjV9/930pmn8YXYYg7Zth36pghMfAgkfAgkfAgkfAgkfAgkfAi3Fa7mzS/A7gEXo/R4fmDz+qQ+B\nhA+BhA+BhA+BhA+BhA+BhA+BhA+BhA+BhA+BhA+BhA+BhA+BhA+BhA+BhA+BhA+BhA+BhA+BhA+B\n/gCQtfi0/ptEVgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fe9ae6b1dd0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction\u001b[91m 2 \u001b[00mwith softmax prob\u001b[94m 0.999862 \u001b[00m\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP4AAAD8CAYAAABXXhlaAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fe9801b4fd0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction\u001b[91m 1 \u001b[00mwith softmax prob\u001b[94m 0.993158 \u001b[00m\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP4AAAD8CAYAAABXXhlaAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAABRFJREFUeJzt3SGLlFscwOH7XrVoE5RtmkREUcFgWLBYxGS22C1+D6vf\nwCYKhgWNgslytxlMBpMGk2KR94ZbNMxZvTOzO+7veeqfM+8pP87C2Xlnmuf5L6Dl74PeALD/hA9B\nwocg4UOQ8CFI+BAkfAg6uu4HTNPkHwXgAMzzPC2aOfEhSPgQJHwIEj4ECR+ChA9Bwocg4UOQ8CFI\n+BAkfAgSPgQJH4KED0HChyDhQ5DwIUj4ECR8CBI+BAkfgoQPQcKHIOFDkPAhSPgQJHwIEj4ECR+C\nhA9Ba/+ZbLrOnTs3nL99+3Y4f/DgwXD+6NGj394T/3HiQ5DwIUj4ECR8CBI+BAkfgoQPQe7xWZur\nV68O5/M8D+cfPnxY5Xb4gRMfgoQPQcKHIOFDkPAhSPgQJHwIco/P2ly5cmU4//Lly3D+/PnzVW6H\nHzjxIUj4ECR8CBI+BAkfgoQPQcKHIPf4LOXixYsLZ/fv3x+uffz48aq3wy9y4kOQ8CFI+BAkfAgS\nPgQJH4KED0Hu8VnK+fPnF85OnDgxXPvkyZNVb4df5MSHIOFDkPAhSPgQJHwIEj4ECR+Cpr1+o3zp\nB0zTeh/AgXrz5s3C2alTp4ZrL126NJzv9d59xuZ5nhbNnPgQJHwIEj4ECR+ChA9Bwocg4UOQ7+Mz\ndPbs2eH82rVrC2fv3r0brnVPf3Cc+BAkfAgSPgQJH4KED0HChyDhQ5B7fIZu3Ljxv9d++vRphTth\nlZz4ECR8CBI+BAkfgoQPQcKHIOFDkHt8hvZ69/3Iw4cPV7gTVsmJD0HChyDhQ5DwIUj4ECR8CBI+\nBE3zvN6fr5+mab0PYCnXr18fznd2dobz9+/fL5xtb28P13779m04ZznzPE+LZk58CBI+BAkfgoQP\nQcKHIOFDkPAhyPfx427evDmcnzx5cjh/+fLlwpl7+s3lxIcg4UOQ8CFI+BAkfAgSPgQJH4Lc48dd\nvnx5ON/rfQ3Pnj1b5XbYJ058CBI+BAkfgoQPQcKHIOFDkPAhyHv1D7mtra3hfHd3dzj//PnzcH7h\nwoXf3hP7w3v1gZ8IH4KED0HChyDhQ5DwIUj4EOT7+IfcvXv3hvPTp08P5y9evFjhbtgUTnwIEj4E\nCR+ChA9Bwocg4UOQ8CHIPf4hd+bMmaXW7/V9fP5MTnwIEj4ECR+ChA9Bwocg4UOQ8CHIPf4hd/v2\n7aXW7+zsrGgnbBInPgQJH4KED0HChyDhQ5DwIUj4EOQe/w+3vb09nG9tbS31+fM8L7WezeTEhyDh\nQ5DwIUj4ECR8CBI+BAkfgtzj/+Hu3LkznB85cmQ4393dHc5fv37923ti8znxIUj4ECR8CBI+BAkf\ngoQPQcKHIPf4G+748ePD+a1bt5b6/KdPnw7n379/X+rz2UxOfAgSPgQJH4KED0HChyDhQ5DwIWha\n93vTp2nyYvYlHDt2bDh/9erVcP7x48fh/O7du8P5169fh3M21zzP06KZEx+ChA9Bwocg4UOQ8CFI\n+BAkfAhyjw+HlHt84CfChyDhQ5DwIUj4ECR8CBI+BAkfgoQPQcKHIOFDkPAhSPgQJHwIEj4ECR+C\nhA9Bwocg4UOQ8CFI+BAkfAgSPgQJH4KED0HChyDhQ5DwIUj4ECR8CDq6D8/4Zx+eAfyGaZ79fD3U\n+FMfgoQPQcKHIOFDkPAhSPgQJHwIEj4ECR+ChA9Bwocg4UOQ8CFI+BAkfAgSPgQJH4KED0HChyDh\nQ9C/99GWzptMWZgAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fe9800e7cd0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction\u001b[91m 0 \u001b[00mwith softmax prob\u001b[94m 0.999947 \u001b[00m\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP4AAAD8CAYAAABXXhlaAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fe980092f10>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction\u001b[91m 4 \u001b[00mwith softmax prob\u001b[94m 0.999807 \u001b[00m\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP4AAAD8CAYAAABXXhlaAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\noH8AWb/kHGeekgcAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fe98016fa50>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction\u001b[91m 1 \u001b[00mwith softmax prob\u001b[94m 0.996322 \u001b[00m\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP4AAAD8CAYAAABXXhlaAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAABUxJREFUeJzt3S9vFVkcgOF7yQICWVsUkgRVTANYFAoMH4IvQPopMDRo\nBLIaLIqQxSARhCBJoK7866zezd5zQyelpe/z2F/mzJiXQ3LuTJfTNC2AlnMn/QDA7yd8CBI+BAkf\ngoQPQcKHIOFD0F/HfYPlcumHAnACpmlarprZ8SFI+BAkfAgSPgQJH4KED0HChyDhQ5DwIUj4ECR8\nCBI+BAkfgoQPQcKHIOFDkPAhSPgQJHwIEj4ECR+ChA9Bwocg4UOQ8CFI+BAkfAgSPgQJH4KED0HC\nhyDhQ5DwIUj4ECR8CBI+BAkfgoQPQX+d9ANwdt25c2c439vbG84fPHgwnO/u7g7nh4eHw3mZHR+C\nhA9Bwocg4UOQ8CFI+BAkfAhaTtN0vDdYLo/3BpyojY2NlbM3b94Mr718+fJwvu4c/tKlS8P5wcHB\ncH7WTdO0XDWz40OQ8CFI+BAkfAgSPgQJH4KED0Hex2eWW7durZxtbm4Or133G5Jnz54N51+/fh3O\nWc2OD0HChyDhQ5DwIUj4ECR8CBI+BDnHZ+jixYvD+cOHD1fOlsuVr4MvFov15/hPnz6ddT2r2fEh\nSPgQJHwIEj4ECR+ChA9Bwocg39VnaGtrazh/9erVkdf+8ePHcH7hwoUjr43v6gP/IXwIEj4ECR+C\nhA9Bwocg4UOQ9/EZunv37nA+eud+3W9Enj9/fqRnYj47PgQJH4KED0HChyDhQ5DwIUj4EOQcn6Gb\nN28O56Oz+m/fvg2v3dnZOdIzMZ8dH4KED0HChyDhQ5DwIUj4ECR8CPJd/bjt7e3h/OXLl0de+/Pn\nz8P5xsbGkddmPd/VB/5F+BAkfAgSPgQJH4KED0HChyDv48ddv359OB99N3+xGL+P//jx4yM9E8fP\njg9Bwocg4UOQ8CFI+BAkfAgSPgQ5x4/b2toaztd9r+HLly8rZ7u7u0d6Jo6fHR+ChA9Bwocg4UOQ\n8CFI+BAkfAhyjn/G3bhxYzi/f//+rPX39/dXzj5+/DhrbY6PHR+ChA9Bwocg4UOQ8CFI+BAkfAhy\njn/Grfsb9OfOzfu3/8WLF7Ou52TY8SFI+BAkfAgSPgQJH4KED0HChyDn+GfcvXv3Zl0/+m7+YrFY\nPHnyZNb6nAw7PgQJH4KED0HChyDhQ5DwIUj4ELRc9/fPZ99guTzeG8Rtbm4O5+/fvx/O172P//bt\n2+H82rVrwzknZ5qm5aqZHR+ChA9Bwocg4UOQ8CFI+BAkfAjyPv4fbnt7ezif+938vb29WddzOtnx\nIUj4ECR8CBI+BAkfgoQPQcKHIOf4f7iNjY1Z13/69Gk4f/To0az1OZ3s+BAkfAgSPgQJH4KED0HC\nhyDhQ5Bz/D/c7du3Z13/4cOH4Xx/f3/W+pxOdnwIEj4ECR+ChA9Bwocg4UOQ8CHIOf4pd/78+eH8\nypUrs9Y/ODgYzr9//z5rfU4nOz4ECR+ChA9Bwocg4UOQ8CFI+BDkHP+UOzw8HM5fv349nF+9enU4\nf/fu3S8/E38+Oz4ECR+ChA9Bwocg4UOQ8CFI+BDkHP+U+/nz53C+s7MznE/TNJyv+x0AZ5MdH4KE\nD0HChyDhQ5DwIUj4ECR8CFquO+edfYPl8nhvAPyvaZqWq2Z2fAgSPgQJH4KED0HChyDhQ5DwIUj4\nECR8CBI+BAkfgoQPQcKHIOFDkPAhSPgQJHwIEj4ECR+ChA9Bwocg4UPQ7/gz2X//hnsAv+DYv6sP\nnD7+qw9Bwocg4UOQ8CFI+BAkfAgSPgQJH4KED0HChyDhQ5DwIUj4ECR8CBI+BAkfgoQPQcKHIOFD\nkPAh6B8jwreYC6Vo2QAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fe9a80b1490>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction\u001b[91m 4 \u001b[00mwith softmax prob\u001b[94m 0.996765 \u001b[00m\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP4AAAD8CAYAAABXXhlaAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fe98018ae50>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction\u001b[91m 9 \u001b[00mwith softmax prob\u001b[94m 0.973803 \u001b[00m\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP4AAAD8CAYAAABXXhlaAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fe978102e50>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction\u001b[91m 5 \u001b[00mwith softmax prob\u001b[94m 0.992624 \u001b[00m\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP4AAAD8CAYAAABXXhlaAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nZgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fe978028d10>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction\u001b[91m 9 \u001b[00mwith softmax prob\u001b[94m 0.995477 \u001b[00m\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP4AAAD8CAYAAABXXhlaAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nfAgkfAj0L55dWFzecy4GAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fe964548e90>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Define operations: a softmax probability, and the prediction by the network\n",
    "with tf.variable_scope('analysis'):\n",
    "    softmax = tf.nn.softmax(logits=net)\n",
    "    prediction_label = tf.argmax(net,1)\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "# Dump some output images\n",
    "prob_array, pred_label, = sess.run([softmax,prediction_label],feed_dict={images: mnist.test.images,keep_prob:1.0})\n",
    "for index in range(10):\n",
    "    print 'Prediction\\033[91m',pred_label[index],'\\033[00mwith softmax prob\\033[94m',prob_array[index][pred_label[index]],'\\033[00m'\n",
    "    plt.imshow(mnist.test.images[index].reshape([28,28]).astype(np.float32),cmap='gray',interpolation='none')\n",
    "    plt.axis('off')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training speed \n",
    "The last training loop took about 23 seconds, which was not very fast (though that's relative). In fact, the most of time consumed was due to creating tensorboard log recorded every 20 steps. For the last exercise in this notebook, let's take out log creation and compare the speed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0, training accuracy 0.12\n",
      "step 100, training accuracy 0.94\n",
      "step 200, training accuracy 0.94\n",
      "step 300, training accuracy 0.92\n",
      "step 400, training accuracy 0.98\n",
      "step 500, training accuracy 0.96\n",
      "step 600, training accuracy 0.96\n",
      "step 700, training accuracy 0.92\n",
      "step 800, training accuracy 0.96\n",
      "step 900, training accuracy 0.9\n",
      "step 1000, training accuracy 1\n",
      "step 1100, training accuracy 0.96\n",
      "step 1200, training accuracy 0.98\n",
      "step 1300, training accuracy 0.98\n",
      "step 1400, training accuracy 1\n",
      "step 1500, training accuracy 0.98\n",
      "step 1600, training accuracy 0.96\n",
      "step 1700, training accuracy 0.94\n",
      "step 1800, training accuracy 1\n",
      "step 1900, training accuracy 1\n",
      "test accuracy 0.9801\n",
      "7.02695202827\n"
     ]
    }
   ],
   "source": [
    "# Let's time this\n",
    "import time\n",
    "start = time.time()\n",
    "\n",
    "# Ready! initialize and train for 5000 steps\n",
    "sess.run(tf.global_variables_initializer())\n",
    "for i in range(2000):\n",
    "\n",
    "    # Fetch data of 50 images\n",
    "    batch = mnist.train.next_batch(50)\n",
    "\n",
    "    # Every 100 steps, compute & print the accuracy of the network's prediction\n",
    "    if i % 100 == 0:\n",
    "        train_accuracy = sess.run(accuracy, feed_dict={images: batch[0], labels: batch[1], keep_prob: 1.0})\n",
    "        print('step %d, training accuracy %g' % (i, train_accuracy))\n",
    "    \n",
    "    # Every step run training!\n",
    "    sess.run(train_step, feed_dict={images: batch[0], labels: batch[1], keep_prob: 0.5})\n",
    "\n",
    "# Compute & print out the final accuracy\n",
    "batch = mnist.test.next_batch(200)\n",
    "print('test accuracy %g' % sess.run(accuracy, feed_dict={images: batch[0], labels: batch[1], keep_prob: 1.0}))\n",
    "print(time.time() - start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7 seconds ... which is more than factor of 3 faster than the last training loop! So, as said, most time was spent writing a training log during the last attempt.\n",
    "\n",
    "## Closing remark\n",
    "In this notebook we trained a very simple, 2 layers convolutional neural networks followed by 2 fully connected layers. This worked fairly well for MNIST digit classification. As a homework assignment, you might want to try:\n",
    "\n",
    "* Make a simple neural network (NN) with 2 fully connected layers. Train and see how well it works.\n",
    "* Get a digit image of 1, and use numpy operations to \"shift\" the location of digit to left or right. Compare how well CNNs vs. NN can handle the translation!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
